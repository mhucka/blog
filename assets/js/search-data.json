{
  
    
        "post0": {
            "title": "A synopsis of Named Data Networking (NDN)",
            "content": "Introduction . The network architecture that underlays today’s Internet is largely based around the concept of moving packets of data from one explicitly identified host to another. It was designed and developed at a time in history when most computers were stationary, limited in number, far in-between, and trusted each other. Internet Protocol (IP), the predominant networking protocol supporting the Internet, is concerned with addressing hosts and sending data packets between them—a pairwise point-to-point communications approach that made sense in the landscape of users and applications that existed in the 1970’s. However, modern uses of the Internet have evolved from their origins, and today, content references and distribution make up the majority of internet traffic. The best estimate in mid-2017 puts the number of visible web pages at nearly 50 billion (van den Bosch et al., 2016), while in terms of data transmitted over the Internet, global IP traffic was estimated at 1.2 zettabytes in 2016 and is predicted to reach 3.3 zettabytes per year by 2021, with IP video traffic estimated to comprise 82 percent of that (Cisco, 2017). “In such a content-centric worldview, what a person wants, rather than where it is located, is what matters most; content, rather than the server on which content resides, becomes the starting point” (Kurose, 2014). . The fact that most modern applications are not well matched to IP’s host-oriented architecture was already apparent to network architects two decades ago. This led to efforts by researchers and funding agencies to explore new architectural designs in the early 2000’s. One outcome of this work was the research on Content-Centric Networking (Stanik, 2009), which later led to the Named Data Network (NDN) project (Jacobson et al., 2009; Jacobson et al., 2012). . NDN changes the focus of network services from delivering packets to a destination (the basic IP approach), to requesting data by name. Architecturally, NDN puts named data chunks at the “thin waist” of the network protocol stack, replacing host-addressed IP packets. (See the next figure.) The name in an NDN packet can refer to anything: a file, a data segment from a movie, a data stream endpoint from an environmental sensor, a command to control something, etc. Data chunks themselves are signed digitally, with a certification scheme that allows a consumer to verify that the content was produced by the expected source and that the data has not been tampered with. . No discussion of Internet protocols is complete without the canonical hourglass diagram of the network stack. (Left) Today&#39;s IP-based Internet architecture. (Right) The NDN-based network architecture. The &quot;thin waist&quot; part of NDN consists of data chunks; this replaces the Internet Protocol packets in the regular IP stack. (Figure based on diagram by Jacobsen et al., 2012.) During normal operation, a user application generates requests for content and these requests are forwarded between NDN routers in a network until they reach the relevant content source(s). The nature of how data is packaged in NDN (discussed below) allows content can be cached or delivered from anywhere else, too. The provision for the network infrastructure to cache content as a by-product of its operation offers the potential for content to be found and reused closer to user destinations, thereby reducing network load and response latency. The nature of NDN data also also the content itself to be secured—not by securing the communication channel (which is how IP does it) but by securing the chunks of data. This is fundamentally a better match to the needs of today’s Internet applications. . General system components and operation . NDN communication is initiated when a consumer makes a request for data. In the NDN protocol, a request for data is expressed by broadcasting an Interest Packet on a network interface connected to an NDN network. Any node in the network having data that satisfies the Interest can respond with a Data Packet. A Data Packet is only transmitted in response to an Interest Packet, and it consumes that Interest, thereby maintaining a flow balance. . Interest Packets primarily consist of a name and optional selectors to specify more precisely the data being requested; Data Packets consist of a name, some metadata, digital signature data, and the content requested (Jacobson et al., 2012). As discussed further below, names in NDN are typically structured hierarchically. A Data Packet “satisfies” an Interest if the name in the Interest is a prefix of the Name in the Data Packet; in other words, if the Data is in the subtree implied by the name in the Interest. . Illustration of packets in NDN. Routers forward Interest Packets to producers that can potentially answer the requests. In the general case, a producer sends back to the consumer by retracing the path through the network in reverse; however, caching may alter this path in practice. . NDN router architecture . To support the operation of NDN networks, there are three main architectural elements in NDN routers: a content store, a pending interest table, and a forwarding information base. . The Content Store (CS): An Data Packet in NDN is idempotent, self-identifying, and self-authenticating. This means that any given data chunk can be cached and served to multiple consumers. The content store in an NDN router is used to cache data temporarily that the router sees in response to Interest Packets, so that if the same Interest Packet is requested again in the future by other consumers, the router can provide the data directly without routing the request to the source again. This increases data sharing, reduces retrieval time and reduces bandwidth consumption. . | The Pending Interest Table (PIT): To track requests for data that have not yet been satisfied, an NDN router maintains an entry for each incoming Interest Packet until its corresponding Data Packet arrives or the time-to-live parameter on the request expires. In NDN, only Interest Packets are routed, and this tracking of packets in effect leaves a trail of breadcrumbs from requester to producer. The PIT entries are used to forward Data Packets back to consumers, thus following the breadcrumbs. PIT entries are erased after they are used to forward a matching Data Packet. . | The Forwarding Information Base (FIB): An NDN router tracks reachable destinations and the next hops in the network using an FIB. The contents of the FIB are created according to a routing protocol used by an NDN network. Unlike the equivalent in IP routing, an FIB entry has a list of outgoing interfaces rather than a single one because in NDN, multiple sources can be queried for data simultaneously. . | . The following diagram illustrates the basic processes that take place in an NDN router in response to Interest Packets and Data Packets. The top half shows an Interest Packet arriving at a router. If the router finds an entry for the named data item in its CS, it returns the data on the interface through which the Interest Packet arrived. Otherwise, it next checks the PIT to see if something else has already requested the same data. If it finds a match in the PIT, it adds the interest of the new request to the list of requesters in the PIT entry. When a Data Packet matching an Interest Packet eventually arrives in response to the request, it is forwarded to multiple requesters based on the list in the PIT (a process known as interest aggregation). Finally, if no PIT entry is found, the Interest Packet is passed to the FIB, which performs longest-prefix match (LPM) on the name to look for a network interface that can answer the request. The router forwards the Interest Request to the next hop based on the most specific match in the FIB, and also creates a new PIT entry to identify the router interface on which the interest is pending. If no suitable match is found in the FIB, then the Interest Packet is flooded to all the outgoing interfaces or else deleted (depending on the router’s policies). . Illustration of the main components of an NDN router. The top half illustrates the actions when an Interest Packet reaches the router; the bottom half represents the actions when a Data Packet arrives. (Diagram based on figure by Saxena et al., which in turn is based on figures from NDN project reports (Saxena et al., 2016; Zhang et al., 2014).) The bottom half of the diagram shows the process when a Data Packet arrives at the router. The process is simpler because Data Packets are not routed in NDN networks—they only follow the chain of PIT entries back to requester(s). Upon an incoming Data Packet, a router first searches its PIT for entries that requested the same data item. If a match is found, the data is passed to all interfaces mentioned in the list, and (depending on the router’s caching policy) also cached in the CS. If no matching PIT entry is found, it means the data was unsolicited and the Data Packet is dropped. . Data naming in NDN . Interest and Data Packets do not carry any host or interface addresses. A “name” in NDN is a sequence of bit strings. The interpretation of names is application-dependent, although the components are usually structured hierarchically and often interpretable as character strings such as “/edu.institution/something/somethingelse”. Hierarchical names of the form “/x/y/z” resemble web URLs, and so are likely to be familiar and convenient for human users to work with; however, the names are opaque to NDN, and delimiters are not part of the names and not included in packet encodings. All that NDN routers do is pay attention to is the structure of names. Name components may even be encrypted. . The components in names are used to perform hierarchical prefix matching. The first or highest-level component of names are globally-routable names (i.e., namespaces). These may each be controlled by separate entities, or multiple namespace prefixes may be controlled by the same entity, and the entities may be distributed on a global network or they may be local (e.g., local servers, local sensors that produce data, etc.) Globally-available data must have globally-unique names, but local communications involving local routing can rely on local names. . Routers forward Interest Packets to data producers based on the name of the data in the packet, and Data Packets are returned based on the PIT information in routers established when the packet was forwarded from router to router to (eventual) producer. Generally, one Interest Packet results in one Data Packet, but fetching large objects that may be divided into multiple Data Packets requires other approaches. One option is for consumers and producers to agree on a dynamic naming scheme that can be used by applications to allow consumers to construct names deterministically without having seen the entire data ahead of time. Another option is to use interest selectors in Interest Packets. The NDN packet format defines a number of operators that can be used in interest selectors to specify wildcard matches against data name suffixes and/or child elements, and this can be used to request multipart data such as might happen when requesting video streams. . Caching . An important aspect of NDN operation are the processes of caching data at network nodes. Implementing cache support in NDN requires the solution of two main problems (Saxena et al., 2016), cache placement and cache update, both of which continue to be active research areas today. As its name implies, the topic of cache placement is concerned with storing copies of data: where to cache them in the network, how many copies to cache, how to coordinate caching policies between routers, and so on. Conversely, cache update is concerned with the procedures implemented by routers for updating the content held in caches. Issues that enter into the equation include the popularity of any given piece of content. . Several benefits accrue from caching. First, content can be uncoupled from producers, so that producers do not have to be accessed (or even accessible) every time a consumer asks for particular content—potentially eliminating a single point of failure. Caching performed by a node in the network can help cope with intermittent network connectivity; mobile nodes can act as content routers between physically disconnected networks. Second, it reduces load on the producer by making content available from multiple sources (i.e., not just the producer but cache sources). Third, if managed properly, it can support more efficient multicasting to many consumers. And fourth, caching can reduce network load and network latency (Saxena et al., 2016). . Forwarding . As mentioned above, the operation of an NDN network involves queries by consumers, in the form of Interest Packets being passed around the network until they reach a consumer, who (in the generic case) respond with a Data Packet that is passed back to the consumer. This operation requires network routers to support efficient interpretation of request and data packets and manage PIT and FIB tables. Since routers need to process a very high number of network transactions per second, the development of approaches for scalable forwarding has been an important topic in NDN development. . The challenge in forwarding packets lies in NDN’s use of unbounded names rather than fixed-length host addresses. This means that routers need to perform character-based prefix matching; at the same time, they also need to manage millions or billions of entries in their tables in order to route network traffic. Operations need to be performed with low latencies, which requires the fastest memory accesses possible, but processors typically have limited amounts of fast on-chip RAM. Thus, operation and management of the PIT table can become a bottleneck. A variety of approaches are being pursued to solve this problem (Saxena et al., 2016). . A separate concern in implementing NDN routers is the FIB table used for Interest Packet forwarding. Once again, the data structures and hardware used to implement the FIB need to be efficient and scale to a large number of constantly-changing entries. This has led to work on both strategies and hardware implementations. On the strategy front, implementing efficient forwarding approaches requires addressing issues of discovering and selecting paths for Interest Packets, congestion control, and more. On the hardware front, work in this area has focused not only on CPU-based approaches but also GPU-based and FPGA-based solutions. . Security and privacy . The architecture of IP was not designed with an eye towards secure data distribution. NDN is fundamentally different and directly implements content-based security: “protection and trust are properties of the content itself, not of the connections over which it travels” (Jacobson et al., 2012). In NDN, every datagram is cryptographically authenticated. This not only avoids some of the existing issues of host-based trust and security; it means that NDN provides end-to-end security between content producer and consumer, and data in NDN is secure at rest, not just during transmission. . Data security . Every Data Packet in NDN is digitally signed using public key cryptography, allowing a receiver to verify the authenticity of the data. The NDN data packet format includes a field for the signature, and a field called the key locator. The key locator is the name (in the NDN sense) of the public key that was used to create the signature. In NDN, a key is another packet of named NDN content, and like all NDN data, it is signed too, to verify it. A consumer can follow the chain of keys to a root key to authenticate any Data Packet. This arrangement provides authentication (a recipient can verify the content was created by a known sender) and data integrity (a recipient can be sure the content has not been modified). The digital signing of packets not only means that data is secured while it is in transmission: since NDN Data Packets are also units of storage, the scheme protects data at rest. Cached data and stored data are secured as a consequence. . Producers can chose the signature algorithm. Verification of signatures may involve multiple rounds of fetching and decryption to get to follow the chain of signatures to a recognized signature; a way to cope with the inherent performance issues is for an NDN-based system to cache validated certificates for some amount of time. . The content of Data Packets is up to applications, and the contents could also be encrypted in addition to the Data Packet itself being signed. . Network security . Some types of network attacks that are common against today’s network infrastructure are mitigated or eliminated by the virtue of how NDN works. For example, port scans of given hosts to find running services is not feasible in NDN: there are no hosts in the same sense as there are in TCP/IP. The equivalent attack would require some kind of “name scan” in order to guess what services might be provided under a given namespace, but since there can be an essentially infinite number of names, the attack is impractical. . Some types of attacks are still possible. For example, bandwidth depletion attacks involve flooding a victim with requests for content. This is a potential attack approach in NDN, but it is mitigated by the fact that once retrieved, content will be cached by intermediate routers between an attacker and providers for a target namespace. . Some new types of attacks are introduced by NDN. One example is interest flooding, in which attackers unleash a high number of unsatisfiable Interest Packets, causing routers to process a large number of packets (including potentially filling their PIT with fake requests). Another potential attack is content cache poisoning, in which an attacker seeks to overload router caches with corrupted Data Packets possessing fake signatures. While fakes can be detected, the overhead of processing them may impact router and network performance. These new types of attacks are under study by NDN developers. . Privacy of communications . A final area of development concerns privacy of communications. The nature of the threat is different from current host-based networking: data itself can be secured as described above, and since NDN does not have destination hosts, the question of who is consuming what content is much more difficult to answer. However, certain kinds of analysis could still compromise privacy. An example is that the names of data being requested could perhaps be used via some kind of correlation analysis to infer producers and/or consumers. . Additional reading . The following list of materials, in the order given here, may help readers become more familiar with NDN. . Edens and Scott’s 2017 article in IEEE Spectrum (Edens &amp; Scott, 2017) is an excellent, non-technical and highly readable overview of content-centric networking. Though the authors scarcely mention NDN by name, everything they describe is true of NDN. | The survey by Xylomenos et al. (Xylomenos et al., 2014) is one of the best and most understandable, moderately-technical explanations of information-centric networking in general. It covers several efforts including NDN, and while it is slightly dated (being published in 2014), it is still worth reading to understand the context of NDN. | Jacobson et al.’s 2012 CACM paper is slightly out of date with respect to the state of NDN today, but it is a good and concise explanation of all the essential components of the NDN architecture (Jacobson et al., 2012). | Saxena et al.’s 2016 paper is the most comprehensive survey of work on NDN so far (Saxena et al., 2016). It gathers into one place a summary and references to work on all aspects of NDN, and provides helpful taxonomies of different areas of work. (Note that the abstract may leave a poor impression about the quality of the writing, but the rest of the paper is considerably better.) | The paper by Yu et al. explains in good detail how security and trust are expected to work in NDN (Yu et al., 2015). | The IETF Internet Draft document draft-rahman-icnrg-deployment-guidelines-00 from the Information-Centric Networking Research Group (Rahman et al., 2018) provides the most up-to-date description of potential deployment scenarios for NDN. | References . van den Bosch, A., Bogers, T., &amp; de Kunder, M. (2016). Estimating search engine index size variability: a 9-year longitudinal study. Scientometrics, 107, 839–856. https://doi.org/10.1007/s11192-016-1863-z | Cisco. (2017). The Zettabyte Era: Trends and Analysis [White paper]. https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/vni-hyperconnectivity-wp.html | Kurose, J. (2014). Information-centric networking: The evolution from circuits to packets to content. Computer Networks, 66, 112–120. https://doi.org/10.1016/j.comnet.2014.04.002 | Stanik, J. (2009). A Conversation with Van Jacobson. Queueing Systems. Theory and Applications, 7(1), 9. https://doi.org/10.1145/1508211.1508215 | Jacobson, V., Smetters, D. K., Thornton, J. D., Plass, M. F., Briggs, N. H., &amp; Braynard, R. L. (2009). Networking named content. Proceedings of the 5th International Conference on Emerging Networking Experiments and Technologies (CoNEXT’09), 1–12. | Jacobson, V., Smetters, D. K., Thornton, J. D., Plass, M., Briggs, N., &amp; Braynard, R. (2012). Networking named content. Communications of the ACM, 55(1), 117–124. https://doi.org/10.1145/2063176.2063204 | Saxena, D., Raychoudhury, V., Suri, N., Becker, C., &amp; Cao, J. (2016). Named Data Networking: A survey. Computer Science Review, 19, 15–55. https://doi.org/10.1016/j.cosrev.2016.01.001 | Zhang, L., Afanasyev, A., Burke, J., Jacobson, V., Claffy, K., Crowley, P., Papadopoulos, C., Wang, L., &amp; Zhang, B. (2014). Named data networking. ACM SIGCOMM Computer Communication Review, 44(3), 66–73. https://doi.org/10.1145/2656877.2656887 | Edens, G., &amp; Scott, G. (2017). The packet protector. IEEE Spectrum, 54(4), 42–48. https://doi.org/10.1109/MSPEC.2017.7880457 | Xylomenos, G., Ververidis, C. N., Siris, V. A., Fotiou, N., Tsilopoulos, C., Vasilakos, X., Katsaros, K. V., &amp; Polyzos, G. C. (2014). A Survey of Information-Centric Networking Research. IEEE Communications Surveys &amp; Tutorials, 16(2), 1024–1049. https://doi.org/10.1109/SURV.2013.070813.00063 | Yu, Y., Afanasyev, A., Clark, D., Claffy, K., Jacobson, V., &amp; Zhang, L. (2015). Schematizing Trust in Named Data Networking. Proceedings of the 2nd ACM Conference on Information-Centric Networking, 177–186. https://doi.org/10.1145/2810156.2810170 | Rahman, A., Trossen, D., Kutscher, D., &amp; Ravindraw, R. (2018). Deployment Considerations for Information-Centric Networking (ICN) (draft-irtf-icnrg-deployment-guidelines-00; Number draft-irtf-icnrg-deployment-guidelines-00). IETF. https://tools.ietf.org/pdf/draft-irtf-icnrg-deployment-guidelines-00.pdf |",
            "url": "https://mhucka.github.io/blog/computing/2018/02/24/ndn-synopsis.html",
            "relUrl": "/computing/2018/02/24/ndn-synopsis.html",
            "date": " • Feb 24, 2018"
        }
        
    
  
    
        ,"post1": {
            "title": "A summary of IPFS",
            "content": "Introduction . IPFS aims to solve several problems at once: efficient and secure decentralized storage, content-oriented addressing instead of location-based addressing, and immutable data. IPFS has some superficial similarities to NDN (Jacobson et al., 2012; Saxena et al., 2016), especially in the way that both aim to change the host-based addressing and point-to-point communications approaches of today’s internet; however, they are markedly different in their approaches. . The IPFS project began in 2014, initially designed by software developer Juan Benet (Benet, 2014) and now being developed by a commercial venture, Protocol Labs as an open-source project. It bills itself as the “Distributed Web”, reflecting its emphasis on replacing current web protocols with a content-addressed, distributed storage scheme. Much like NDN, IPFS changes the focus of network services from delivering packets to a destination (the basic approach with TCP/IP), to requesting content by an identifier. To achieve its goals, IPFS amalgamates several technologies. Storage is achieved by distributing content across a network of peers; distribution is achieved using a peer-to-peer protocol derived from BitTorrent. Content is “named” by the value computed by a hash function from the content itself; this hash value is a unique fingerprint for every block or sequence of bytes. (This is in contrast to NDN, which uses separate, hierarchically-structured names.) The system is fully decentralized, with no central authority. It does not require DNS: content is resolved over the peer-to-peer network using a distributed hash table and does not involve DNS name lookups. Nor does IPFS require host authority certificates. . Like NDN, IPFS promises to lighten network loads by caching copies of content around a network; retrieval can thus potentially use the node closest to a client, rather than from a specific server. Unlike NDN, IPFS does not seek to change the Internet architecture at the level of packet protocols and network routing. In fact, current IPFS implementations leverage existing Internet protocols to implement an overlay network, and so IPFS can exist on top of many different underlying networking protocols. . IPFS is not the only effort of its type today. Dat (Ogden et al., 2017), MaidSafe (Lomas, 2016), Storj (Wilkinson et al., 2014), Blockstack (Ali et al., 2016) and Freenet (Clarke et al., 2002) are probably the closest related systems. In all cases, these projects aim to provide decentralized content storage that is addressed by keys rather than traditional locations, and is resistant to censorship and network fragmentation. Each participating node’s storage and bandwidth are harnessed to provide, collectively, a large distributed storage network. Some of the efforts such as Freenet emphasize anonymity and security as a philosophical and technical stance; others such as IPFS emphasize the potential for permanence of content. . General system components and operation . The basic function of IPFS is to provide client applications with an API that is oriented towards storing and retrieving content identified by alphanumeric text strings. These text strings are computed directly from the content itself: given a block of data, a mathematical formula known as a cryptographic hash function is used to produce an identifier string that is unique to that block of data, such that a change of even one bit in the data will lead to a different value. The value is known as a hash in IPFS parlance. If we think of IPFS as a large key-value storage system, then a hash is used as a key and content is stored as a value associated with that key, but with a twist: the key is derived from the content itself. . Deriving an identifier for a block of data in this way is pivotal to several benefits offered by IPFS. The first is natural deduplication: given that any given block of data is uniquely identified by its hash value, and no two blocks of data will have the same hash unless they are identical, it follows that IPFS only needs to store a given block once. The IPFS infrastructure distributes copies around a network for data redundancy and efficiency reasons, but this is unlike the situation with regular file systems in which multiple copies of the same content can exist under different names. A second benefit is content permanence: writing new data to the system never changes the content associated with a given hash, because the hash is always unique. Content stored in the system is thus immutable, which means it does not matter who serves it or how. This affords the opportunity for efficient caching and preservation. A third benefit is verifiability: any client can verify that a given piece of content is the expected content by recomputing the hash value for the data it gets back from a retrieval request. . Network architecture . The network topology of IPFS is a flat, unstructured, fully decentralized network in which peers collectively provides a global storage and lookup service. More specifically, the network implements the S/Kademlia distributed hash table (DHT) algorithm (Baumgart &amp; Mies, 2007; Maymounkov &amp; Mazières, 2002), which defines how individual nodes find out about peer nodes as well as what to do when new nodes join the network and existing nodes disappear or fail. . At the peer level, the fundamental operations defined by a DHT are surprisingly few (Zhang et al., 2013): essentially, there is only “ping” (to verify a node is still accessible), “store” (store a key-value pair), “find node” to return the node(s) that are closest to the requested key, and “find value”, which is like “find node” but returns a value. Key-value pairs in a DHT are stored in such a way that values are distributed around the network based on node identifiers; the identifiers are chosen from the same space as the keys, thus making individual nodes responsible for storing the values associated with particular keys. In Kademlia, keys and node identifiers are 160-bit strings. A distance metric is defined on identifier space (unrelated to physical network topology) such that proximity is assessed in the order of high bits to low bits. When a new value is to be stored in the network, it is hashed to produce a 160-bit key, and a certain number of the leading bits of the key is used to derive the address of a node responsible for storing all values that begin with those same leading bits. . To enable peer-to-peer communications between heterogeneous devices with potentially different network underlays, the IPFS project developed a network communications stack that can seamlessly use multiple transports to provide an abstracted peer-to-peer connection API. This is implemented in the form of a library called libp2p. The library supports many transports including TCP, UDP, SCTP, UDT, WebRTC, SSH and other; multiple authentication protocols including TLS and SSH; self-describing wire protocols; NAT traversal; and encrypted channels. libp2p is agnostic with respect to transport protocols; it does not assume IP, and could be implemented on top of NDN. In addition, libp2p can multiplex multiple communication operations over a single connection. For example, it can multiplex multiple connections per peer, multiple network listening interfaces, multiple streams per protocol, and more. Finally, once connected, connections can be reused from both ends. . IPFS can operate in partitioned networks. A retrieval operation will work as long as any node in a network has the sought-after content. Thanks to the uniqueness of hashes, rejoining partitioned networks naturally allows content previously stored separately to be available across the joined network, without fear of different content being stored under the same identifier while the networks were partitioned. This has benefits for operation in areas with poor network connectivity and networks of mobile nodes. . Data storage . Exchange of data between peers is accomplished using a custom protocol called BitSwap, based on the BitTorrent protocol. The basic operation rests on a kind of “marketplace of data blocks” scheme, in which nodes have lists of data blocks they can offer and lists of blocks they want to acquire. Unlike BitTorrent, the blocks in question are not limited to coming from a single source. As discussed above, content stored on IPFS is uniquely identified and immutable, which means a peer does not care about the source of a given data block – every block is uniquely identified by its hash, so all a peer cares about is retrieving the block corresponding to a given hash. In fact, the individual pieces of a large file (which will be decomposed into individual blocks) may be retrieved from different peers in the global IPFS network. To make all of this work, the BitSwap protocol defines procesures for tracking block exchanges via distributed ledgers, as well as incentives for nodes to share data even when they do not seek to acquire anything in return. . The combination of DHT and BitSwap provides IPFS with a robust and efficient scheme for storing and distributing blocks of data across a large number of peers. On top of this infrastructure, IPFS implements a Merkle DAG: a directed acyclic graph of objects where the links between objects are the hashes of the content stored in the nodes. The following is a simple illustration of this concept: . Illustration of a DAG created between objects identified by hashes. . The basic IPFS object structure is surprisingly simple: . type IPFSLink struct { Name string; // Name or alias of this link Hash Multihash; // Cryptographic hash of target Size int; // Total size of target } type IPFSObject struct { links []IPFSLink; // Array of links data []byte; // Opaque content data } . Objects stored in IPFS Merkle DAGs are themselves data blocks with hashes, which is how IPFS stores structured objects (such as directories of files) and large files that have been decomposed into smaller blocks. The format of the data stored in the data field is up to applications. . Merkle trees are used in other systems, including the Git revision control system, Dat, Bitcoin, and file systems such as ZFS. The extension of the Merkle tree to a DAG used in IPFS is a generalization of that format. It is a very flexible structure that allows many other types of data to be mapped directly to the IPFS Merkle DAG. For example, Linked Data triples can be easily represented, as can cryptocurrency blockchains, key-value data stores, and others. . Naming data in IPFS and IPNS . Unlike some other content-oriented storage schemes such as NDN, which allow users and applications to choose names in essentially any way they desire, in IPFS the “names” are unique fingerprints derived from the data itself via a cryptographic hashing function. Anyone can compute the hash of a data block and ascertain whether the given hash matches the block – affording direct verification of data integrity. However, the scheme as described so far is still incomplete: . IPFS has values are long random strings of characters such as XLF2ipQ4jD3UdeX5xp1KBgeHRhemUtaA8Vm. These are unsuitable for direct human use. . | Real-life scenarios do require the ability to associate different data with the same names, or (to put it another way) for a name to be reused. For instance, when publishing a news website at a given address, it is important for an author to be able to show updated content without forcing users to access a different address every time. . | Without a way to define mutable data somehow, it would be impossible to communicate new data within the framework of IPFS itself: new content (or rather, the hashes for new content stored in the system) would have to be announced through some other means. To understand why, consider something as simple as a text file containing a list of hashes for data. Writing any changes to the file will lead to a new hash for the file itself, and thus a reader will need to know the new hash in order to read the modified file – but how will the reader find out the new hash for the modified file? . | . This conundrum is solved by a combination of things: a scheme for self-certified names, and IPNS, the InterPlanetary Name Server, for mapping names to hashes. The approach for self-certifying names is borrowed from another system called SFS (Self-certifying File System). The scheme in IPFS for names uses public-key cryptography to sign the combination of a name and the hash to which the name refers, and publish this mapping via IPNS. Clients can query IPNFS with a name and get back a result that has been signed with a digital signature, allowing a client to verify that the hash mapped to the name has not been altered. . Data availability and persistence . The nodes in an IPFS network are the providers of storage. However, it is not some kind of large cloud-based storage system. When a node stores content in IPFS, it advertises to the network that it has content – but that content is on its local drive. IPFS does not require every node to store all of the content ever published; in fact, it is an explicit design requirement that an IPFS client does not automatically download content that a user does not request. Nodes are not required to “store other people’s stuff”, which is important for legal reasons. Still, this raises a question: how does content end up persisting? . The first element of the solution is the notion of pinning content, which is how a client can tell IPFS it wants certain data to persist rather than allow it to be cleared at certain times (such as when a client reboots). The second element is that nothing prevents users from creating nodes that keep their content online, nor in establishing collaborations with other nodes to host content on certain conditions. Users can offer to host content, and in a large-enough network, content will end up being copied and replicated on multiple nodes. There is also an ongoing effort to define ipfs-cluster, a tool to coordinate content distribution between IPFS nodes that will make this process easier. Finally, a third major element of achieving persistence in IPFS is Filecoin, discussed below. . Filecoin . Filecoin is a sister effort to IPFS aimed at providing incentives for users to provide storage. Today, there exists a vast amount of unused disk space around the world in data centers and individual user’s hard disks. Filecoin is an electronic currency similar to Bitcoin that lets users offer a portion of their disk space and be paid for fulfilling storage requests on the Filecoin market. Filecoin’s proof-of-work function involves proof-of-retrievability, which requires nodes to prove they store a particular file. . The use of Filecoin incentivizes users to store data in IPFS. If successful, it has the potential to discourage users from merely leeching the services provided by other users, and effectively solve a problem that has plagued many distributed peer-to-peer systems: leveraging humans’ natural self-interest to encourage users to provide services for the good of the overall system. . Additional reading . Being a relatively young project not based in an academic environment, there are unfortunately no publications describing IPFS in a comprehensive and up-to-date manner. The following list of materials may help readers learn more. . The most detailed explanation of the entire architecture is Benet’s 2014 white paper (Benet, 2014), but it is fairly technical and still unfinished in places. | A reasonably high-level introduction to IPFS can be found in a 2015 blog posting by Drake (Drake, 2015). | Another blog posting by Lundkvist and Lilic (Lundkvist &amp; Lilic, 2016) also provide useful high-level descriptions of IPFS. | After that, the best places to find out more are to read discussions on the IPFS forums at https://discuss.ipfs.io/. | References . Jacobson, V., Smetters, D. K., Thornton, J. D., Plass, M., Briggs, N., &amp; Braynard, R. (2012). Networking named content. Communications of the ACM, 55(1), 117–124. https://doi.org/10.1145/2063176.2063204 | Saxena, D., Raychoudhury, V., Suri, N., Becker, C., &amp; Cao, J. (2016). Named Data Networking: A survey. Computer Science Review, 19, 15–55. https://doi.org/10.1016/j.cosrev.2016.01.001 | Benet, J. (2014). IPFS – content addressed, versioned, P2P file system. Computing Resources Repository, arXi:1407.3561. | Ogden, M., Robinson, D., Hand, J., McKelvey, K., &amp; Buus, M. (2017). Dat Project. In Dat Project. https://datproject.org/ | Lomas, N. (2016). After a decade of R&amp;D, MaidSafe’s decentralized network opens for alpha testing. In TechCrunch. https://techcrunch.com/2016/08/12/after-a-decade-of-rd-maidsafes-decentralized-network-opens-for-alpha-testing/ | Wilkinson, S., Boshevski, T., Brandoff, J., &amp; Buterin, V. (2014). Storj: A Peer-to-Peer Cloud Storage Network. Citeseer. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.693.785 | Ali, M., Nelson, J., Shea, R., &amp; Freedman, M. J. (2016). Bootstrapping trust in distributed systems with blockchains. USENIX Magazine, 41(3). https://www.usenix.org/system/files/login/articles/login_fall16_10_ali.pdf | Clarke, I., Miller, S. G., Hong, T. W., Sandberg, O., &amp; Wiley, B. (2002). Protecting free expression online with Freenet. IEEE Internet Computing, 6(1), 40–49. https://doi.org/10.1109/4236.978368 | Baumgart, I., &amp; Mies, S. (2007). S/Kademlia: A practicable approach towards secure key-based routing. 2007 International Conference on Parallel and Distributed Systems, 1–8. https://doi.org/10.1109/ICPADS.2007.4447808 | Maymounkov, P., &amp; Mazières, D. (2002). Kademlia: A Peer-to-Peer Information System Based on the XOR Metric. Peer-to-Peer Systems, 53–65. https://doi.org/10.1007/3-540-45748-8_5 | Zhang, H., Wen, Y., Xie, H., &amp; Yu, N. (2013). Distributed Hash Table: Theory, Platforms and Applications. Springer. https://doi.org/10.1007/978-1-4614-9008-1 | Drake, K. (2015). HTTP is obsolete. It’s time for the distributed, permanent web. In Neocities. https://ipfs.io/ipfs/QmNhFJjGcMPqpuYfxL62VVB9528NXqDNMFXiqN5bgFYiZ1/its-time-for-the-permanent-web.html | Lundkvist, C., &amp; Lilic, J. (2016). An Introduction to IPFS. In Medium. https://medium.com/@ConsenSys/an-introduction-to-ipfs-9bba4860abd0 |",
            "url": "https://mhucka.github.io/blog/computing/2018/02/19/ipfs-summary.html",
            "relUrl": "/computing/2018/02/19/ipfs-summary.html",
            "date": " • Feb 19, 2018"
        }
        
    
  
    
        ,"post2": {
            "title": "SoManyLanguages! SoManyLicenses!",
            "content": "For one of the projects I’m working on, we need comprehensive lists of programming languages, software licenses, and several other software-related topics. We could not find any existing single sources that were comprehensive enough. Wikipedia has a large number of pages about languages, it’s true, but it’s not complete. (Our project has been mining GitHub repositories and we found many others.) . Faced with this need, we had to create our own lists as start points, but we can’t hope to build up or maintain these lists solely ourselves. This is a case where we need community participation. . For this reason, I started two projects on GitHub: . SoManyLanguages | SoManyLicenses | . They are both similar in using plain old Markdown tables to provide the data, and GitHub’s slick facilities for fork/pull/propose changes make it super simple and easy for anyone to propose an update to the lists. And it’s been working – people have already contributed updates. . So please go over to the repositories listed above and help flesh out the lists! Even a simple link addition is enough to get your name into the list of committers/contributors :-). .",
            "url": "https://mhucka.github.io/blog/computing/2016/01/18/so-many-languages.html",
            "relUrl": "/computing/2016/01/18/so-many-languages.html",
            "date": " • Jan 18, 2016"
        }
        
    
  
    
        ,"post3": {
            "title": "Annotating PDFs on an iPad",
            "content": "In my continuing quest for greater efficient, I recently went back to an approach that I tried and abandoned before: to annotate a PDF on an iPad using an electronic pen. This time I had a much better iPad (an iPad Air), a much better pen (the Jot Dash), and a much better app for annotations (Notability). It all worked, but I’m not sure if it was the most efficient approach in terms of time use. . The experience was pleasant enough. Notability is a great application, and the Jot Dash – with a much finer point than the pens I’ve tried before – worked very well. The procedure was simple: put the PDF in Dropbox on my computer, import the PDF from Dropbox in Notability on the iPad, annotate, then upload the annotated result to Dropbox and open it again on my laptop. I was pleased that I could read the result in Acrobat: there were no issues that I could see with the annotations produced on the iPad. . However, I think it didn’t save me any time over what it would have taken to do annotations in Acrobat directly on my laptop. I think the ability to read the PDF on an iPad and use a pen gave it all an air of familiarity, like using real paper held in my hands and writing on it with a pen, but I had to rewrite some things to make my writing on the iPad more legible, and writing by hand for me is slower than typing. If I had used Acrobat annotations in the usual way, I am sure I would have been able to enter the information more quickly. Even things like circling something or pointing to something in the text with a line and arrow could be done in Acrobat, so there are no real limitations of using Acrobat for adding basic comments on a paper. Plus, there is additional time wasted in putting the document into Dropbox, downloading it to the iPad app, and then uploading the result to Dropbox. . Now, compared with another the alternative of writing on real paper with a real pen and then scanning the result, I suspect the virtual paper &amp; pen approach is more efficient overall. First, you can erase and change your annotations (which is messier with pen on paper, and in the long run can lead to more time wasted on clarifying the results with whoever you’re sending the feedback to), and second, you don’t have to do a scanning step, which can take some time and be frustrating even if you have portable scanner with you. . The part that is difficult to quantify is whether using the tablet and being able to use that more familiar and intuitive method (virtual pen on paper) let me think more deeply or even just differently from when I am reading a PDF on a laptop. That in itself might be worth the time wasted due to the inefficiencies of the process. . So in a nutshell: the approach is slower for me at this point, but being able to treat the PDF almost as a paper in my hand and use a virtual pen may have some advantages that can’t be obtained by typing comments on a laptop. .",
            "url": "https://mhucka.github.io/blog/general/2015/11/23/annotating-pdfs-on-ipads.html",
            "relUrl": "/general/2015/11/23/annotating-pdfs-on-ipads.html",
            "date": " • Nov 23, 2015"
        }
        
    
  
    
        ,"post4": {
            "title": "Math shortcuts for the LaTeX-inclined",
            "content": "When writing technical notes, you sometimes need to write mathematical symbols and expressions. Most technical people who come from a math or computer science background learn how to write math expressions in TeX or LaTeX, but when writing in something other than a TeX/LaTeX-enabled application (say, a note-taking program like Evernote), you may not have direct access to TeX/LaTeX and you many not want to paste an image like something produced by LaTeXiT. . I want to share an approach to easily typing many mathematical symbols using the same command sequences you would use for TeX or LaTeX documents—but without actually using TeX/LaTeX. The approach is completely keyboard driven (no mousing around) and conceptually simple: use a text expansion program to define text abbreviations that are exactly the LaTeX commands for mathematical symbols, and make the abbreviations expand to those symbols when triggered. This way, you don’t need to learn different shortcuts or abbreviations, and it works in any application where you can type symbol characters (which on the Mac, is nearly everywhere). You can define the abbreviations with a text expansion program such as TextExpander, or a program such as KeyboardMaestro that offers this facility. (The latter is what I actually use, because I already use KeyboardMaestro for other things and didn’t want to run yet another background program—I have too many things running on my Mac already.) . In more detail, here is how to set it up: . In text expansion program, define a group for the abbreviations. In KeyboardMaestro (and I am sure in Textpander too), groups of abbreviations can be toggled active/inactive via a keyboard shortcut. I use the sequence command-option-control-&#39; (that’s a normal single quote at the end), which is a key sequence that’s easy to type and that I don’t use for anything else. The first press of that key combination activates the abbreviations group, and a second press deactivates it. | Within that group of abbreviations, define shortcuts for all of the LaTeX symbols and other things that you want to use. In KeyboardMaestro, this requires defining an abbreviation that inserts text, and the text to be inserted is the symbol that the LaTeX command represents. So in other words, define alpha to insert the character “α”, define beta to insert the character “β”, and so on, all the way through to special symbols such as cup for “∪” (set intersection), cap for “∩” (set union), sum for “∑”, and so on. For Greek letters, I use capitalized names for the capitalized variants of the letters: omega for ω and Omega for Ω, etc. | When you want to access the symbols while writing, activate the abbreviations group using the keyboard shortcut you use (command-option-control-&#39; in my case), then proceed to write things like &quot;P(A cap B) = P(A)P(B)&quot; (which is what you would type when using LaTeX) and have it instantly turn into more readable text such as “P(A ∩ B) = P(A)P(B)” without actually running LaTeX (or having to click around in the Mac OS X “Special Character…” palette, or do other things). | You may wonder about the need for defining a keyboard sequence for toggling the active status of the abbreviations group. You need that because otherwise, when you actually did want to write a LaTeX document, it would be impossible unless you could turn off the abbreviations! So, you want to be able to toggle them on and off easily. . This approach does have its limitations, of course. The most significant is that it does not let you write full mathematical formulas. Nonetheless, I personally find that for many things this scheme gets me very far. I often combine this approach with LaTeXiT for bigger equations, and use this scheme for writing intermediate text, such as explanatory text that refers to the symbols in the equations created in LaTeXiT. More often, I just need to write notes about something and only need to use a Greek letter or a very short expression, for which the full power of LaTeXiT is overkill. Oh, I suppose if I could remember the Mac keyboard shortcuts for Greek letters and if they didn’t interfere with other shortcuts that I’ve defined in QuicKeys or KeyboardMaestro, then I could just use them. However, I find it easier to use the notation I already know (namely, LaTeX) than to learn new keyboard shorcuts. . Final bonus benefit: if you don’t know the LaTeX sequences for various symbols, this approach also helps you learn them by giving you more opportunities to practice their use. .",
            "url": "https://mhucka.github.io/blog/computing/2014/12/30/math-shortcuts.html",
            "relUrl": "/computing/2014/12/30/math-shortcuts.html",
            "date": " • Dec 30, 2014"
        }
        
    
  
    
        ,"post5": {
            "title": "Git prepare-commit-msg hook for running Python pep8",
            "content": "Code style guidelines are important for enabling collective code ownership: . “[T]he team adopts a single style so that they can freely work on any part of the system. They don’t have to obey personal standards when visiting “another person’s” code (silly idea, that). They don’t have to keep more than one group of editor settings. They don’t have to argue over K&amp;R bracing or ANSI, they don’t have to worry about whether they need a prefix or suffix wart. They can just work. When silly issues are out of the way, more important ones take their place.” — Tim Ottinger and Jeff Langr . It’s sometimes hard to stick to a consistent style, though, especially when you’re switching from one project to another, each with their own collaborators and possibly different styles. One way to help code follow guidelines is to automatically format code (for instance, upon check-in, or on some regular interval), but in my opinion, this is too dictatorial; moreover, there are legitimate cases were you want to format the code in a certain way so that the reader can understand it better, and doing that may break the coding guidelines. Such custom formatting would get obliterated by auto-formatting the code. . Instead, I prefer the idea of running code through format checkers that inform you when the code breaks conventions (whatever your conventions are) but does not change the code for you. This helps you by pointing out where you might have unintentionally strayed from your project’s conventions, yet doesn’t destroy carefully-constructed parts of the code that knowingly do not follow the conventions. . When using git, you can achieve this by taking advantage of its prepare-commit-msg hook system. For a Python project I’m involved with, I developed a short script that runs pep8, a Python code checker. The hook script runs pep8 on Python files that are about to be committed. If pep8 reports anything, the report is inserted as comments into the commit message template. The person about to commit code will see the comments and can correct the issues before they continue with the commit. . This approach was the best that I could come up with in order to handle the following situation: if you use an editor like Emacs, and your commit message is thrown into an editing buffer, you may not see messages printed on stdout or stderr by the git commit command. By putting the pep8 output into the commit message template, it helps ensure you see it before committing. . To use it, first copy the script from my GitHub repository to some location in your git repository that you use for developer scripts or tools. Next, look inside the script for the following line near the top, . IGNORE=E221,E226,E241,E303,E501 . and modify the values to fit your project’s coding conventions. See the pep8 documentation for an explanation of the codes. The settings in my copy of the script reflect my personal preferences; your project’s conventions may differ, so make sure to adjust this as you see fit. (And it goes without saying that everyone in a project needs to use the same settings.) . Finally, to use it on a project, every member of your project needs to do the following: . copy the script to the .git/hooks/ subdirectory of their local git repository | rename the script to prepare-commit-msg | make it executable (chmod +x on the file) | (The reason every person needs to do this individually is that git hooks are only run in a user’s local repository.) The code assumes that pep8 is installed on the user’s computer. If that’s not the case, then make sure everyone also downloads and installs it on their computers. . If you have improvements to this or a better way to do it entirely, please do let me know. .",
            "url": "https://mhucka.github.io/blog/computing/2014/12/06/git-prepare-commit-msg-hook.html",
            "relUrl": "/computing/2014/12/06/git-prepare-commit-msg-hook.html",
            "date": " • Dec 6, 2014"
        }
        
    
  
    
        ,"post6": {
            "title": "Adapting Emacs's vc diff for word-oriented diffs",
            "content": "A few years back, I changed how I work with text files (including LaTeX files) in Emacs: instead of using hard newlines to format paragraphs into lines, I use soft wrapping, and do not insert hard newlines except to break paragraphs. This change was driven by the fact that, except for software development work, most modern editing environments (and most of what my colleagues send) assume soft-wrapped paragraphs. This was an annoying change at first, but I worked out how to set up soft wrapping in Emacs so that I no longer really notice the difference. However, one problem that remained involved the use of version control systems such as git: most of those systems are line-oriented by nature and they show differences in a line-oriented way by default. The resulting diffs are basically unreadable if the source text does not contain hard line breaks. . Here is how I set up git and Emacs’s standard vc version control package for word-oriented diffs. I put the following code in my .emacs file. First, make sure to load the vc Emacs package: . (require &#39;vc) . Next, set vc-git-diff-switches to the following value to tell git to use word-oriented diffs instead of line-oriented diffs: . (setq vc-git-diff-switches &quot;--word-diff&quot;) . If you do the above, you’ll get word oriented diffs from Emacs’ vc-diff, but IMHO the format is very difficult to read: it’s expressed as in-line text codes, with {+ ...} and [- ... ] markers inserted by git diff to indicate what has been added or removed between the two versions of a file. These are hard for the human visual system to pick out quickly. What you really want is to use color to distinguish changes. I searched and found someone else’s solution to colorize the diff output, but I couldn’t get it to work well in my environment. So, I rewrote it. My version uses Emacs’s advising functionality to modify Emacs’ vc-diff function to edit the git diff output and colorize it using Emacs overlays. The colors are set by the two variables vc-diff-added-face and vc-diff-deleted-face. Here’s the code: . (make-face &#39;vc-diff-added-face) (make-face &#39;vc-diff-deleted-face) (set-face-background &#39;vc-diff-added-face &quot;lawn green&quot;) (set-face-background &#39;vc-diff-deleted-face &quot;RosyBrown1&quot;) (defadvice vc-diff (after vc-diff-advice last activate compile) (save-window-excursion (with-current-buffer &quot;*vc-diff*&quot; (let ((inhibit-read-only t)) (goto-char (point-min)) (while (re-search-forward &quot; ({ ( ) | [ (- ) )&quot; nil t) (let* ((front-start (match-beginning 0)) (front-end (match-end 0)) (addition-p (match-beginning 2)) back-start back-end) (cond (addition-p (re-search-forward &quot;}&quot; nil t) (setq back-start (match-beginning 0)) (setq back-end (match-end 0))) (t (re-search-forward &quot; - ]&quot; nil t) (setq back-start (match-beginning 0)) (setq back-end (match-end 0)))) (if addition-p (overlay-put (make-overlay front-end back-start) &#39;face &#39;vc-diff-added-face) (overlay-put (make-overlay front-end back-start) &#39;face &#39;vc-diff-deleted-face)) ;; Make sure to delete back-to-front, because the text will shift. (delete-region back-start back-end) (delete-region front-start front-end) (goto-char front-end))) (goto-char (point-min)) )))) . Here’s an example of what the output looks like in an Emacs buffer. You can see the effect is to color deletions in red and additions in green. . This example uses code and not plain text, to demonstrate that the change does not make code diffs any worse. (And actually, I think the result is better for code diffs too.) The result is basically equivalent to what you would get with git diff —color-words in a terminal, but designed for Emacs. . By the way, I’m still using Aquamacs 2.x, which is based on Emacs 23, so while I think the code above should work in later versions of Emacs or Aquamacs, I have not tested it there. . If you find problems or improvements, please let me know! .",
            "url": "https://mhucka.github.io/blog/computing/2014/11/18/emacs-vc-diff.html",
            "relUrl": "/computing/2014/11/18/emacs-vc-diff.html",
            "date": " • Nov 18, 2014"
        }
        
    
  
    
        ,"post7": {
            "title": "Hummingbird in the garden",
            "content": "This is just a random picture taken earlier today. A hummingbird sat on a bush in the back of our garden, trying to hide. These little birds are so tiny! Even though I was probably 10 feet away and had a 600mm equivalent telephoto lens, I still had to do a little bit of cropping on the image. .",
            "url": "https://mhucka.github.io/blog/photography/2014/07/29/hummingbird-in-the-garden.html",
            "relUrl": "/photography/2014/07/29/hummingbird-in-the-garden.html",
            "date": " • Jul 29, 2014"
        }
        
    
  
    
        ,"post8": {
            "title": "The three types of file backups you really should have",
            "content": "Most of us working in the computational science and information technology areas use our laptops as our primary computers. They’re absolutely essential to our work (and even our non-work lives), and any problems can completely disrupt our productivity. This is why it’s a constant surprise to me to discover how many computational scientists do not seem to have a regular, high-frequency, computer backup solution in place. . I learned the hard way that even a relatively new laptop’s disk can fail unexpectedly. In 2008, the disk on my then-only 7 months old laptop failed with a hardware failure. One minute I’m working right along, the next minute the disk is unreadable. My backups at the time were 12 days old. Why were my backups so old? Because I got lax in doing them. I thought, “This is too new – it’s not in danger of failing so soon. I’ll set up a better backup scheme next week.” That was a disruptive and expensive lesson. I ended up using a disk recovery service that charged $1500 and took over two weeks to recover the contents of the disk. (Apparently what they do is disassemble the disk drive in a clean room, take the platters off and put them in another drive casing, and somehow manage to make things work enough to be able to read data off.) . That experience led to the three-tiered approach I use today. It is as automated as I could make it, because anything that requires manual intervention is in danger of being neglected eventually. (And odds are, the time you neglect it is exactly when you need it the most.) Here follows an explanation of my approach and some suggestions. . Automated, hourly, incremental backups . Hourly backups are important for at least two reasons: (1) to avoid disruption of your work if a sudden hardware failure or other problem arises, and (2) so that if you delete a file accidentally, you can recover a relatively recent version of it. Incremental backups are important because you want a system that backs up only the changes to your files, not entire files; this saves disk space and time. . Since I use Mac OS X, I use Time Machine to perform incremental hourly backups. It is truly one of the best features of Mac OS X; a set-up-and-forget incremental backups system that doesn’t even require you to worry about stopping the backup process before unmounting your disks – you can unmount them and take your laptop at a moment’s notice, and the next time you plug your disk(s) back in, Time Machine will restart and continue its work. Time Machine is built into Mac OS X, but there are other similar solutions for other operating systems. . An important consideration to making this practical is to have a sufficiently fast interface on the backup disk drive. I find that a disk connected over USB 2.0 is not fast enough, even if the disk itself is fast. When you try to back up a disk with 300 GB or more, the step of comparing your disk to the backups to identify the changes can take hours over a USB 2.0 interface. A FireWire 800 interface has become the minimum necessary in my case; USB 3.0 is even better. . Because I work from home much of the time, as well as work from two offices, I put backup disks in all locations. Time Machine lets you configure multiple disks and will use whichever one is available automatically. . By the way, if you’re a programmer, you may think that keeping your files in git or Mercurial or a similar repository is enough. I would argue no, for two reasons. First, do you put everything in your version control system? Everything including your mail files, the configuration and preferences files for various programs you use, etc.? I would wager that the answer is no. Second, do you check in all the changes every hour? Again, I bet the answer is no. . Automated, daily, bootable mirror on external disk . Having backups is good, but what will you do if the main disk in your computer fails and you cannot boot? “Drive to the computer store” may not be an option if you are traveling, but even if it is, recovering or reinstalling a disk from backups is a time-consuming affair. A better and simpler solution is to have a disk you can boot from and that contains a recent copy of your entire disk. That way, you can not only become operational again almost immediately; the recovery from hourly backups will go much faster. . To implement this, I use a small portable disk and a cloning utility (Carbon Copy Cloner) that is set to mirror the contents of my laptop drive to the external disk on a nightly basis. When at home, I leave this disk plugged into my laptop; when traveling, I make sure to plug it in when in my hotel room in the evening. If a catastrophe happens and my laptop’s drive fails, I can plug in the external mirror and boot from it, and have a system that is no more than 24 hours of out date. . If you have the option to use an external disk that can potentially be swapped for your computer’s internal drive, then that’s the best solution. Then, if your internal drive fails, you can pull the mirror disk out of its enclosure and put it inside your laptop. Voilà, you have a working computer again. However, not all modern laptop drives are removable (oh, hi, MacBook Air!), so this is not always possible. The crucial thing in any case is to make sure the external mirror disk is something you can boot from. . Of course, nightly backup operations will not run if your computer is turned off. I always leave my laptop running at night (whether at home or traveling) so that the disk mirroring and off-site backups (below) are performed. . Note that the bootable mirror accomplishes a different goal than hourly backups: the Time Machine backups are not bootable, and unless you carry your Time Machine backups disk with you wherever you go, you may not be able to access your backups anyway. If you’re traveling, the only hourly periodic backups you are likely to have are network-based (see the next item), and you may not be able to access them if you can’t boot your computer. . Automated, daily, off-site, networked backups . All of the above are still not enough. It is crucial to have one more backup scheme in place: off-site, networked backups. If your portable mirror disk fails while you’re traveling, you don’t all backups to stop being performed as a result; if your laptop and your luggage are stolen while you’re traveling, you want access to your files; if your house or place of work is robbed or experiences a fire, you want a copy of your entire disk that is not co-located with your backup disk, your mirror disk and your laptop. . To implement this, I personally use CrashPlan, an excellent service that has really solved over-the-net, cross-platform backups for me. Previously, I developed a home-grown solution using rsync, but CrashPlan is even better and simpler. CrashPlan offers free plans as well as paid plans; you can use the free plan with a backup server of your own and achieve nightly remote backups from wherever you are. I set up a server in my office that acts as a group CrashPlan server for myself and my team. I have it set to back up every 24 hours, and since it works over the net, it works from wherever I am, whether at home or at work or traveling. . Once again, to make this work, I leave my computer running overnight and connected to a network, both at home and in hotel rooms while traveling. . A lot of people use an alternative scheme, such as putting their most important files in Dropbox. In fact, I tried this for a time, too – it seemed like a great idea to put my working set of files (notes, software projects, etc.) in a Dropbox folder, so that I can access them from any computer or device. But I soon stopped. The approach has drawbacks. One is that, since you typically don’t put your entire computer’s disk drive in Dropbox, you are at risk of losing something that you did not think to put in Dropbox. Another drawback is the performance impact of putting high turnover files in Dropbox: every time you touch a file, Dropbox immediately begins to synchronize that file, which uses CPU, disk I/O and network I/O. I only had to compile a large program once to realize the impact is just too much for me. . Dropbox does have the advantage that it records changes almost immediately, so your network copy will be, on average, more recent than a copy made using a periodic backup scheme. This is a tradeoff of computer performance versus backup recency. I chose performance over backup recency because I have the other backup mechanisms in place, but the choice may be different for other people or other uses. (It’s worth noting, however, that if you use the paid version of CrashPlan, you can adjust the backup frequency down to once every 15 minutes if you wish.) .",
            "url": "https://mhucka.github.io/blog/general/2014/02/28/backups.html",
            "relUrl": "/general/2014/02/28/backups.html",
            "date": " • Feb 28, 2014"
        }
        
    
  
    
        ,"post9": {
            "title": "Using an SD flash card for extra storage in a MacBook Pro",
            "content": "One never seems to have enough disk space on laptops, and that’s especially true on Mac OS X systems. It’s surprising how quickly you can use up the space on even a 500 GB SSD drive once you add such things as multiple virtual machines (for software development on Windows and Linux), iTunes (for synchronizing my iPhone and iPad), bibliography management programs with gigabytes and gigabytes of PDFs, years of archives of meetings and workshops, etc. Plus, the operating system’s virtual memory files and sleep image take up a chunk of space. On top of that, you have to leave at least 10% of the disk space free for good file system performance, so you never really get to use the full 500 GB anyway. . I found myself running out of space on a MacBook Pro Retina, and an upgrade to a 1 TB internal disk was just too expensive to justify. An intriguing idea presented itself, however: the rMBP’s have an SD memory card slot, one which I almost never actually use. Couldn’t I just keep an SD flash card plugged in and use it for extra storage? . Well, the problem with regular SD memory cards is that they stick out of the slot. You simply can’t leave a card plugged in all the time, because you’re sure to damage it while moving your laptop around, especially in and out of bags. On the other hand, if you had to keep mounting and dismounting the card and pulling it out and back in, it would just be too much of a hassle and too prone to accidents. If only those SD cards were shorter … Hmm … . So I looked around for shorter SD cards. Turns out short SD cards do not seem to exist, but something else does: an adapter for micro SD cards that fits flush with the exterior of the laptop! I found one product in particular, the MiniDrive, works well enough and suits my needs. I bought it, along with the highest-capacity micro SDXC card available (which is currently 64 GB, though 128 GB are supposed to come in the near future), mounted it, formatted it, and moved various not-so-essential files to it. And it works! . In case any readers are interested in doing the same, here are some noteworthy points: . The first micro SDXC flash card I bought didn’t work properly. This fact was not immediately obvious: I copied files to it seemingly without error, and the files had the correct sizes and properties, and the first few files I verified seemed okay. But files soon ended up containing only zeros inside—something I didn’t realize until the next day when I tried to look at a file. The card was the fastest SanDisk-branded 64 GB micro SDXC I could find. I returned it and bought a Samsung Electronics 64GB Pro microSDXC Extreme Speed Memory Card (MB-MGCGB/AM). It was more expensive than the SanDisk card, but it has been working flawlessly. I have no idea whether the particular SanDisk card I got was a dud (although it turns out reviewers on Amazon do complain about that card being unreliable), but based on this experience I would recommend the Samsung card. Whatever you get, after writing a lot of data to it make sure to test it carefully. | Make sure to format the card to use the Mac OS X Extended Journaled file system format. Do not use the default format, which (IIRC) is exFAT and highly suboptimal for use on Mac OS X. | Be aware that the card’s read/write speeds are much slower than your main disk’s. So, use the card for storing such things as infrequently-used applications, files you want around but don’t access daily, and backups. | Add the volume to your Time Machine backups, so that whatever you put on it will be backed up. | One of the things I moved to the card is my ~/Library/Application Support/MobileSync/Backup directory. I left a symbolic link in its place; this moves the backups from my iPad and iPhone off the main disk, yet still allows iTunes and mobile sync to work normally. It does slow down syncs, but not enough to get in my way. (YMMV of course, so it’s worth experimenting.) . This whole scheme has been working without trouble for me for months now, and has saved 50 GB of space. I still have room for a little bit more on the card. .",
            "url": "https://mhucka.github.io/blog/general/2014/01/25/using-sd-flash-card-for-macbook-storage.html",
            "relUrl": "/general/2014/01/25/using-sd-flash-card-for-macbook-storage.html",
            "date": " • Jan 25, 2014"
        }
        
    
  
    
        ,"post10": {
            "title": "Label your stuff—it just might come back to you",
            "content": "For a number of years, a company called StuffBak has been making a system available for helping lost items get returned back to their owners. The scheme involves a sticker that has a unique identification number together with the StuffBak phone number and web address on it, plus an online registration system. You get a sticker, you register it with StuffBak, you stick it on an item, and if the item is ever lost and someone finds it, hopefully the finder will contact StuffBak, who in turn will contact you. You can offer a reward for the return of the item. It’s a great idea, and apparently it does work. . Here I want to describe a simpler do-it-yourself alternative that has already worked for me once, and I think stands as much or more chance of getting expensive stuff back to you. I use a label maker (one of the many Brother label makers) to print small plastic-covered labels that say the following: “If found, please contact mhucka@caltech.edu”. I put this on the back of my iPhone, my cell phone, the bottom of my laptop, power adapters, laptop video cable adapters, airline power adapter, laser pointer, USB dongles, etc. – anything that’s big enough to put a label on. . The nice thing about modern label makers is that they can print in various font sizes, so you can make very small labels, medium ones, and really big ones. You can also get different label colors: black letters on white background, black letters on transparent background, white letters on black background, and so on. This lets you tailor the label to the particular device and make the label more attractive and discreet. The plastic-coated labels are waterproof too. . I don’t want to engrave most devices because they get replaced after a couple of years, at which time I probably want to sell them or give them to someone. A permanent engraving reduces the value of the device to the next person. Redistributing and reselling devices is an important part of managing your gadget world :-). Plus, if I change email addresses, I can simply replace the labels, which is much easier than dealing with engravings. . Yes, the labels are not permanent. My reasoning is that if someone is the type of person who would return something they found, then they’ll do it, and you just need to give them a way to contact you. Email is better than a phone number, IMHO. On the other hand, if the finder is the kind of person who won’t return something, then nothing you do will make them return it – StuffBak label or no label. Even engravings can be rubbed off, or the affected panel replaced, etc. . And finally – OK, this is probably just wishful thinking on my part – I like to think that something so naively trusting as a simple label and a straightforward request to be contacted conveys a kind of implicit trust in the stranger who finds the item. Maybe it will encourage the finder to do a good deed. . It has already helped a video adapter dongle find its way back to me after a workshop, so I know it can work. .",
            "url": "https://mhucka.github.io/blog/general/2013/12/29/label-your-stuff.html",
            "relUrl": "/general/2013/12/29/label-your-stuff.html",
            "date": " • Dec 29, 2013"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I’m a member of the Caltech Digital Library Development group and have been working on ways to improve the preservation of knowledge and scientific data. My interests include extraction and preservation of knowledge, handwritten text recognition, artificial intelligence, simulation, systems biology, and cognitive science. . After beginning my career first in artificial intelligence and computational neuroscience, I shifted to systems biology in the late 1990s. In collaboration with a superb team of computational scientists and international collaborators, I codeveloped the most widely-used format for exchanging computational models in systems biology, SBML (the Systems Biology Markup Language), as well as MIRIAM (Minimal Information Requested in the Annotation of Models), BioModels.net, the Systems Biology Graphical Notation SBGN, the Systems Biology Ontology SBO, and I helped start BioModels Database. To help support community standards in computational biology, I cofounded COMBINE (the Computational Modeling in Biology Network). Separately, I have also worked on systems for finding software. . Earlier in my career, I co-developed the first version of NeuroML, maintained (for a time) the GENESIS simulation system, worked on cognitive science in the group of Stephen Kaplan, and worked in the group of John Laird on applying the Soar cognitive architecture to robotics. . When not in front of a computer or spending time with family and friends, I enjoy photography, skiing, and scuba diving. .",
          "url": "https://mhucka.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mhucka.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}